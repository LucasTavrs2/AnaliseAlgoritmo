Para começar nosso curso de Análise de Algoritmos, é importante compreender **por que estudamos algoritmos**. Em aplicações reais—seja selecionar anúncios para maximizar receita de uma plataforma, ordenar milhões de inteiros ou encontrar padrões em sequências de DNA—o sucesso depende de escolher ou projetar algoritmos eficientes. A disciplina vai muito além de “dizer o que programar”: trata‑se de aprender técnicas para **projetar novos algoritmos** e analisar rigorosamente seu desempenho.

### Análise de algoritmos e complexidade

Quando analisamos um algoritmo, queremos saber quão rápido ou quanta memória ele consome conforme o tamanho da entrada cresce. A complexidade é medida em termos de **tempo** e **espaço**; o tempo de execução é baseado no número de passos que o algoritmo realiza, independentemente do hardware. Para comparar algoritmos, usamos a notação assintótica. A **notação Big‑O** indica um limite superior para a quantidade de operações que um algoritmo realiza; ela descreve a ordem de crescimento da função de tempo de execução em função do tamanho da entrada. Formalmente, dizemos que f(n) é O(g(n)) se existem constantes c>0 e n0 tal que f(n) ≤ c·g(n) para todo n ≥ n0. Essa abordagem abstrai detalhes de implementação e permite avaliar a **escalabilidade** de algoritmos.

Um exemplo motivador da importância da análise é o problema de **encontrar um número duplicado em um conjunto de 4 bilhões de inteiros**. Uma solução ingênua compara todos os pares de números, o que exige Θ(n²) comparações—complexidade quadrática—e seria impraticável. Uma abordagem melhor consiste em ordenar os números e, em seguida, fazer uma busca binária ou percorrer uma vez o vetor ordenado. Ordenar n elementos com um algoritmo de comparação leva Θ(n log n) operações; no exemplo do slide, reduziríamos o tempo de várias décadas de processamento para apenas alguns segundos. A análise de algoritmos ajuda a prever essas diferenças drásticas.

### Ordenação e limites inferiores

A maioria dos algoritmos de ordenação comuns—como quicksort, heapsort e mergesort—usa apenas comparações entre elementos. Teoricamente, mostra‑se que qualquer algoritmo de ordenação baseado exclusivamente em comparações precisa de, pelo menos, n·log n comparações no pior caso. O mergesort atinge exatamente esse limite em todos os casos (melhor, pior e médio), ou seja, ele é Θ(n log n). Isso garante que não existe algoritmo de comparação geral que seja assintoticamente mais rápido.

### Técnicas de projeto de algoritmos

#### Dividir e Conquistar
A técnica **Dividir para Conquistar** consiste em **dividir** um problema grande em subproblemas menores do mesmo tipo, **resolver** cada subproblema recursivamente e depois **combinar** as soluções. Essa estratégia aparece em algoritmos de ordenação (mergesort, quicksort), busca binária, multiplicação de números grandes e multiplicação de matrizes. Suas vantagens incluem tratar problemas grandes de forma modular, facilitar implementações recursivas claras, permitir paralelismo (subproblemas independentes podem ser resolvidos em paralelo) e levar a algoritmos muito eficientes.

#### Algoritmos Gulosos
Um **algoritmo guloso** (greedy) escolhe a melhor opção local em cada etapa, **sem voltar atrás**, com a esperança de obter uma solução global ótima. Ele é simples de implementar e frequentemente muito eficiente. Contudo, só funciona corretamente se o problema possui **subestrutura ótima**, ou seja, se a melhor solução global pode ser construída a partir das melhores escolhas locais. Exemplos clássicos incluem o problema do troco (quando as denominações de moedas são “bem comportadas”), o algoritmo de Kruskal para árvores geradoras mínimas e a mochila fracionária. Em alguns problemas (como a mochila 0/1) um algoritmo guloso não produz a solução ótima; nesses casos recorremos a programação dinâmica.

#### Programação Dinâmica
A **programação dinâmica** é uma técnica de otimização usada para resolver problemas complexos dividindo‑os em subproblemas menores, **resolvendo cada subproblema apenas uma vez** e **armazenando seus resultados**. Ela é apropriada quando o problema tem subproblemas **sobrepostos** e exibe **subestrutura ótima** (a solução do problema depende das melhores soluções das partes). Ao evitar cálculos repetidos, transforma soluções exponencialmente lentas em algoritmos muito mais rápidos—geralmente polinomiais. Exemplos incluem a sequência de Fibonacci, o problema da mochila 0/1, a subsequência comum máxima (LCS) e a cadeia de multiplicação de matrizes.

#### Grafos e Busca em Profundidade
Grafos modelam relações entre objetos, como pré‑requisitos de disciplinas ou conexões em redes. O **Depth‑First Search (DFS)** é um algoritmo de travessia que visita um vértice, depois explora recursivamente os seus vizinhos até não haver mais vértices a visitar, **marcando cada vértice visitado para evitar ciclos**. Ele é semelhante à travessia pré‑ordem de árvores, mas adaptado para grafos, que podem conter ciclos e componentes desconectados. Começamos a partir de um vértice fonte e exploramos todos os vértices alcançáveis; para grafos desconectados, repetimos até que todos os vértices estejam visitados. O DFS é fundamental para aplicações como detecção de ciclos, topological sorting, identificação de componentes conexos e algoritmos de pontes e articulações.

### Teoria da complexidade: classes P e NP

Além de projetar algoritmos, estudamos a **complexidade computacional**, que classifica problemas de acordo com os recursos necessários para resolvê‑los. Dois conjuntos importantes são **P** e **NP**. A classe **P** contém problemas de decisão cujas soluções podem ser **encontradas em tempo polinomial** em função do tamanho da entrada. A classe **NP** contém problemas cujas soluções, se fornecidas, podem ser **verificadas em tempo polinomial**, mas não se sabe se existem algoritmos que também as resolvam em tempo polinomial. O famoso problema **P versus NP** pergunta se todo problema cuja solução pode ser verificada rapidamente também pode ser resolvido rapidamente; em outras palavras, se P = NP. Essa questão permanece em aberto e tem enorme implicação prática: se P=NP, problemas como o melhor roteiro para visitar todas as capitais brasileiras (análogo ao caixeiro viajante) se tornariam facilmente resolvíveis, mas sistemas de criptografia poderiam ser quebrados. Se P≠NP, sabemos que certos problemas exigem algoritmos exponencialmente lentos e precisamos buscar heurísticas.

### Conclusão e dicas para o curso

Nesta disciplina, abordaremos o **estudo da eficiência de algoritmos** e o **projeto de novas soluções**. Começaremos com fundamentos (complexidade de tempo e espaço, análise assintótica e ordenação), avançaremos para estruturas de dados e grafos, e estudaremos técnicas de projeto como **divide‑and‑conquer**, **greedy** e **programação dinâmica**. Também discutiremos tópicos adicionais como **complexidade computacional** (P vs NP). Para aproveitar o curso ao máximo, é recomendável **ler os materiais previamente**, fazer exercícios de fixação, questionar “por que” cada algoritmo funciona e pensar em possíveis alternativas. Essa curiosidade e prática o ajudarão a não apenas aplicar algoritmos existentes, mas também a criar soluções eficientes para novos problemas.
